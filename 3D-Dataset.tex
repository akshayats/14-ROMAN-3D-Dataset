%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{subfig}

\graphicspath{{images/}}

\title{\LARGE \bf
Preparation of Papers for IEEE Sponsored Conferences \& Symposia*
}

\author{Akshaya Thippur$^{1}$, Rares Ambrus$^{1}$, Kaushik Desai$^{1}$, Adria Gallart$^{1}$, Malepati Sai Akhil$^{2}$, Gaurav Agrawal$^{2}$,\\Mayank Jha$^{2}$, Janardhan HR$^{2}$, Nishan Shetty$^{2}$, Prasad NR$^{2}$, John Folkesson $^{1}$ and Patric Jensfelt$^{1}$% <-this % stops a space
\thanks{*This work was supported by STRANDS}% <-this % stops a space
\thanks{$^{1}$ KTH Royal Institute of Technology
        {\tt\small albert.author@papercept.net}}%
\thanks{$^{2}$ MSRIT Bangalore
        {\tt\small b.d.researcher@ieee.org}}%
}


\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
\begin{quote}\

Humans subconsciously exploit various strong correlations amidst different object instances, classes and between different object classes and scene types when analysing indoor environments. Correlations in naive, logical object co-occurrences have been exploited along with the extraction of vision based object-intrinsic descriptive features in previous research. In this paper, we present several alternative learning techniques to model and make estimates of scenes based on a variety of spatial relations - geometric extrinsic features with different amounts of discretization, which capture \textit{how} the objects co-occur; and compare their efficacy in the context of object classification in real-world table-top scenes. We investigate the possibilities of using such techniques to refine the results from a traditional vision-perception system. We also contribute a unique, long-term periodic, large 3D dataset of 20 office table-top scenes, manually annotated with 18 object classes. Apart from our current comparison, we foresee that the dataset will be useful for applications such as generalized learning of spatial models, learning object data structuring based on semantic hierarchy, learning best suited semantic abstractions and grammar for long term autonomy, ground truth for vision-perception systems etc.

\end{quote}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:Introduction}

Objects pervade human environments. If robots are to perform useful service tasks for humans it is crucial that they are able to locate and identify a wide variety of objects in everyday environments. State-of-the-art object recognition/classification typically relies on the extraction features to be matched against models built through machine learning techniques. As - the number of objects a given system is trained to recognise - increases, the uncertainty of individual recognition results tends to increase as greater number of objects increases the chance of overlapping features existence. The reliability of such recognisers is also affected when used on real robots in everyday environments, as objects may be partially occluded by scene clutter or only visible from certain angles, both potentially reducing the visibility of features for their trained models. In this paper we argue that the performance of a robot on an object recognition task can be increased by the addition of \emph{contextual knowledge} about the scene the objects are found in. In particular we demonstrate how models of the \emph{spatial configuration} of objects, learnt over prior observations of real scenes, can allow a robot to recognise the objects in unseen scenes more reliably. 

Our work is performed in the context of developing a mobile service robot for long-term autonomy in indoor human environments, from offices to hospitals. The ability for a robot to run for weeks or months in its task environment opens up a new range of possibilities in terms of capabilities. In particular, any task the robot performs will be done in an environment it may have visited many times before, and we wish to find ways to capture the contextual knowledge gained from previous visits in a way that enables subsequent behaviour to be improved. The use of context to improve object recognition is just one example of this new robotics paradigm. In this paper we focus on the task of \emph{table-top scene understanding}, and more specifically what objects are present on a table-top. Whilst the objects present on a single table may change in position, their overall arrangement has some regularity over time as influenced by the use to which the table is put. For example, if this table is used for computing, then a (relatively static) monitor will be present, with a keyboard in front of it and mouse to one side. A drink, or paper and a pen, may be within an arms length of the keyboard, as may headphones or a cellphone. This arrangement may vary across different tables in the same building, but the overall pattern of arrangements will contain some structure. It is this structure we aim to exploit in order to improve the recognition of table-top objects, e.g. knowing that the object to the right of a keyboard is more likely to be a mouse than a cellphone. 

As the absolute positions of objects on a table (or their relative positions with respect to some fixed part of the table) is unlikely to generalise across a range of different tables, we are investigating \emph{relational} models of space, i.e. ways of encoding the position of a target object relative to the position of one or more landmark objects. Using a novel data set of table-top scenes (described in Section~\ref{sec:Dataset}), in this paper we explore the performance of a variety of representations for relative object position, plus inference techniquess for operating on them, on the task of table-top scene understanding (Section \ref{sec:Techniques}). In particular we investigate representations that use varying forms of spatial relations, from geometric ones such as distances and angles to more qualitative spatial relations such as \textit{Left} and \textit{Behind} as a means for capturing observations of object configurations over time. The contributions this paper makes are: (1) A novel comparison between mechanisms for representing, learning and inference on object spatial configurations using spatial relations. (2) An evaluation of the use of these mechanisms for augmenting a robot's vision based \textit{perceptual system} (PS). (3) A new large 3D annotated table-top benchmark dataset.

\subsubsection*{Points discussed}
\begin{enumerate}
	\item \textbf{Make this more "Human Oriented"}
	\item Overview of the problem - why?
	\begin{enumerate}
		\item Not only desktops
		\item How do we treat object classification
	\end{enumerate}
	\item Generalize and Transfer Knowledge
	\item How are objects correlated, search becomes easier
	\item Understanding a model for inter object influence for inference
	\item No benchmarks yet or datasets having 3D spatial information
	\begin{itemize}
		\item time
		\item complete scenes
		\item different people
		\item different types of people
	\end{itemize}
	\item need to structure data -- metric is tedious, because of large amounts of data
	\item Contributions:
	\begin{itemize}
		\item A big 3D data set, annotated -- benchmark for results, folding data, classification.
		\item suggest SR and QSR
		\item something that could augment perception systems
	\end{itemize}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
\label{sec:Related Work}

Spatial relations have been used previously to provide contextual information to vision-related work. \cite{MyungJin:CVPR2010}used a hierarchy of spatial relations alongside descriptive features to support multiple object detections in a single image. Spatial relations and contextual information are commonly used in activity recognition from video streams. For example, \cite{Krishna:ECAI2010} demonstrate the learning of activity phases in airport videos using spatial relations between tracked objects, and \cite{Behera2012} use spatial relations to monitor objects and activities in videos of a constrained workflow environment. Recent work has used object co-occurrence to provide context in visual  tasks. Examples in 2D include object co-occurrence statistics in class-based image segmentation~\cite{Ladicky:IJCV2013}; and the use of object presence to provide context in activity recognition~\cite{Li:2012}. However, all this previous work is restricted to 2D images, whereas our approaches work with spatial context in 3D (RGB-D) data. Authors have also worked with spatial context in 3D, including parsing a 2D image of a 3D scene into a simulated 3D field before extracting geometric and contextual features between the objects \cite{Xiao:SIGGRAPH2012}. Our approaches to encoding 3D spatial context could be applied in these cases, and we use richer, structured models of object relations.

Apart from using the statistics of co-occurrence, a lot of information can be exploited from \textit{how} the objects co-occur in the scene, i.e. the extrinsic, geometric spatial relations between the objects. Recent work in 3D semantic labelling has used such geometric information along with descriptive intrinsic appearance features~\cite{Koppula:NIPS2011}. They achieve a high classification accuracy for a large set of object-classes belonging to home and office environments. Scene similarity measurement and classification based on contextual information is conducted by \cite{Fisher:ACMT2011}. They also use spatial information for context-based object search using Graph Kernel Methods. The method is further developed to provide synthetic scene examples using spatial relations \cite{Fisher:ACMT2012}. In~\cite{Aydemir:ICRA2011} spatial relations between smaller objects, furniture and locations is used for pruning in object search problems in human environments. In \cite{Lin:ICCV2013} a technique is developed for automatic annotation of 3D objects. It uses intrinsic appearance features and geometric features and is employed to build an object and scene classifier using conditional random fields. In \cite{kasper:2011} the authors utilise both geometric single object features and pair-wise spatial relations between objects to develop an empirical base for scene understanding. Recent studies \cite{Southey:2007,kasper:2011} compute statistics of spatial relations of objects and use it for conditional object recognition for service robotics. Whilst our techniques are comparable to those in the literature, our contribution comes from the explicit comparison of different representations of spatial context (metric vs qualitative) on a novel, long-term learning task. Additionally our qualitative approach relies on relationships which could be provided through other mechanisms than unsupervised machine learning (e.g. through a human tutor describing a spatial scene), and in this way bootstrap the system using expert knowledge.

Our work is evaluated on a new 3D long-term dataset. Other datasets exist: The \textit{B3DO dataset} \cite{Janoch:ICCV2011} which contains many single-snapshot instances of indoor human environments having a variety in viewpoints, object-classes, scene-classes and instances. This dataset is in the form of RGB and depth image pairs with manual 2D annotations of object classes, capturing many unique scenes with the sole aim of finding more realistic scenes which are difficult for PSs to perform scene classification. \textit{NYU Depth V1-2} \cite{Silberman:ECCV2012} datasets contain different instance examples of object-classes and scene-classes. Each image instance is a combo of synchronous RGB and D images of a different scene-class with semantic annotation provided to every pixel. This dataset is aimed at helping PSs with automatic semantic segmentation and scene classification. The \textit{3D IKEA database} \cite{Swadzba:RAS2012} has been collected using robotic maneuvering in different scene-class instances. The aim is to test scene-classification algorithms based on large furniture level objects. The \textit{WRGBD dataset} \cite{Lai:ICRA2011} is aimed to support object classification methods and contains many instances of isolated objects in .pcd format. Annotation is done by assigning every pixel a correct semantic label.None of these datasets contain periodically collected data or easily usable spatial annotations of objects which are key for long-term autonomous scene-learning, based on spatial relations.

The required regularity in instances and time was the main motivation for the construction of this dataset, as currently available datasets either are of individual objects or singular instances of entire rooms

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{enumerate}
  \item More on dataset papers
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Motivating Scenario}
\label{sec:Motivating Scenario}

We are investigating systems that operate for long periods of time in environments populated by humans. 
As a motivating scenario we will look at security guard in an office building. The robot patrols the working environment and should learn 
models of what the environment normally looks like and what variations there are. In an implementation of such a system the robot tell when 
something differs from the ordinary too much and then raise an alarm. Initially we will focus on desktop scenes. We are interested in models 
for individual desks and as well as general models of desks. Our working hypotheses is that there is some general rules for how desks are 
organized that we want to be able to extract and later exploit when building the models. We expect that object will change place, within certain semantic bounds. Some objects could be missing at times (coffee mug) which is normal, but some other objects are rarely moved (monitor).

Another aim is to be able to transfer knowledge from one environment to the next. This would allow a robot that just entered a new 
environment to be functional from the start. Concretely this would correspond to having a reasonable prior which can then be adapted when 
new observations are available. What information is general and what is environment specific? How do we represent the knowledge in a way that caters for the knowledge transfer, the ability to learn from few samples and adapting existing models? These are some further examples of questions that we want to study.

To study these questions we need data to learn from. The data need to capture both variations across different desks but also over time. None of the datasets available (see Section~/ref{sec:Related Work}) meet these requirements which is the motivation for the work behind the dataset that we present in this paper.

\subsubsection*{Points Discussed}
\begin{enumerate}
	\item This section should stress why it is important to gather to such datasets
	\item what type of data do we really need? - hint it
	\item reference to related work section to show that this dataset is unique.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dataset}
\label{sec:Dataset}

\begin{figure*}
\begin{center}
\includegraphics[height=2.5cm]{David_Mor_131110} \quad
\includegraphics[height=2.5cm]{Nils_Mor_131111} \quad
\includegraphics[height=2.5cm]{Puren_Eve_131029}\\ \smallskip
\includegraphics[height=2.5cm]{David_Eve_131110} \enskip
\includegraphics[height=2.5cm]{Nils_Eve_131111} \enskip
\includegraphics[height=2.5cm]{Puren_Mor_131110}
\caption{Each column shows a different person's office table in top view at two different times. The tables in teh first two columns are captured in the morning and evening of the same day, whereas the table in the last column is captured 12 says apart. We can see distinct differences between different person's desk but there are also many commonalities that a system should exploit. }
\label{fig:Example Scenes}
\end{center}
\end{figure*}

Most of the human outdoor and especially indoor environments, are characterized by a supporting surface on which all relevant objects are placed e.g. Land - buildings, living room floor - furniture, dining table - cutlery and dishes etc. It is hence convenient to perceive a structure for the object arrangement in the human environments to be as if on a "nested table-top system". Table-tops are the most commonly present large objects in any floor map. They provide for a favourable prototypical example for analysis of object organization structure. Moreover, it is highly likely that objects on a "table" have more semantic and organizational correlation amidst them, than when compared to those objects not on that same "table" e.g. \textit{Pen} has more semantic/functional correlation with a \textit{Laptop} (on the table) than a \textit{Couch} (not on that table, here the "table" references to the floor of the room).

With the aim of exploring possibilities to understand, learn and model these organizational formalities amongst objects in human indoor environments, a first, pertinent dataset called \textit{\textbf{3D T}able-T\textbf{o}ps Dataset for Long \textbf{T}erm \textbf{A}utonomous \textbf{L}earning} (3D-TOTAL), has been composed by periodically capturing observations of entire table-tops of a fixed set of researchers at a computer science research facility. A singular observation of a table-top of one person at a single instance in time, captured in image format, is termed as a \textit{scene}.
%% Blind %%
%KTH University.
These scenes have been captured with intervals of few hours, over many days and various instances of object classes. The dataset therefore captures the individual and group variation in object position and pose due to humans and their interaction with the environment. The required regularity in instances and time was the main motivation for the construction of this dataset, as currently available datasets either are of individual objects or singular instances of entire rooms.

It is required for such a long-term autonomous learning to have entire views of the scenes to model the interrelations amidst the member objects. Apart from being variant over object instances across different scenes, the data needed to be periodic over time at a scale of couple of hours so as to capture the individual and group variations in position and pose when there has been regular/irregular human interaction involved. The currently available datasets either are of individual objects or single instances of entire rooms. The required regularity in instances and time was the main motivation for the construction of this dataset (Section~\ref{sec:Related Work}).

\subsection{Dataset Design and Concept}
\label{ssec:Dataset Design and Concept}

%\subsubsection*{Points Discussed}
%\begin{enumerate}
%	\item Why did we collect it the way we did?
%	\item What did we collect?
%	\item Time and people level slicings
%	\item weekends and weekdays
%	\item different times of the day
%	\item We want the data to have these properties and hence we designed the dataset in this way.
%\end{enumerate}
The target research groups to benefit from this dataset are of the kind that develops artificial intelligence for autonomous robots augmenting human activities (especially if they are fatiguing) within indoor human environments. Hence, it becomes essential for robotic learning to pay attention to variances in human environments wrt. scales of time, space and instances in the same space. The dataset has been composed by capturing and manually annotating 3D images of office type table-tops, for a fixed set of people, at fixed times of the day and for a span of many days.

Observing the table-tops of the same set of people at different times of the day gives insight about the daily interactions a human has with his table and over many days gives an understanding of the gradual variances in their table-top setups. If the data is observed for an entire week, including weekends, features in the table-top configurations, that can be used for estimation of the type of the day of the week, can be extracted (e.g.\ Weekdays, Fridays, Weekends). Table-top models can also be learnt for all the people put together -- which gives a gross functional representation of a typical table-top in general for research employees in office environments -- or for individual people which helps to gain functional representations of office table-tops for individuals, seniority, gender and so on (Figure~\ref{fig:Example Scenes}). Finally, when trying to find general models over any of these types of data partition, the model learns to be able to generalize over different instances of a fixed set of object classes. In summary: When the dataset is partitioned in different ways with respect to time, people or instances, it richly yields knowledge and hence representations of table-tops in office environments.

As explained in Section~\ref{sec:Motivating Scenario} our research intentions are to provide intelligence to an long-term operating, autonomous, human activity augmenting robot for indoor human environments. Thusly, the concept of composing the 3D-TOTAL dataset follows naturally from this research motivation.

\subsection{Dataset Realization}
\label{ssec:Dataset Realization}
%\subsubsection*{Points Discussed}
%\begin{enumerate}
%	\item Annotation Tool
%	\item Tools for collecting Asus Scenect
%	\item 
%\end{enumerate}
In 3D-TOTAL, 3D images of scenes were captured regularly at 3 times a day for 19 days for 20 people. Each scene has been manually annotated to obtain information about common objects-of-interest generally and regularly present, when observed across the many scenes.

The data was collected using the \textit{SCENECT} software \cite{Buerkler:Online2012} and an \textit{Asus Xtion Pro} RGB-D camera. There is one 3D colour point cloud per scene (.pcd format). Every scene is a reconstructed version of the raw data stream obtained by manual detailed scanning of a table-top with real time visual feedback using SCENECT. The software has in-built -- real-time sampling, registration and de-noising algorithms to output the final high resolution point cloud.

The scenes were recorded as periodically as possible and at three fixed time instances of the day: \emph{Morning} (09:00 hrs), \emph{Afternoon} (13:00 hrs) and \emph{Evening} (18:00 hrs). Scenes contain tables of 20 different people collected over 19 days including weekends. A \textit{Scene\_ID} is attached to each scene to indicate who the table belongs to and the date and time of the recording. These Scene\_IDs help in partitioning the dataset with respect to time of the day \{Morning, Afternoon, Evening\}, person \{Anna, Bob, Carl, \dots\}, or day \{2013-11-01, 2013-11-06, 2013-11-13, \dots\}.

A 3D annotation tool was developed for manually segmenting out objects-of-interest from the point clouds. On average, 12 different objects were labelled, including repeating instances of the same object class, per scene depending upon feasibility and occurrence. The objects belong to the following super set - \{Mouse, Keyboard, Monitor, Laptop, Cellphone, Keys, Headphones, Telephone, Pencil, Eraser, Notebook, Papers,  Book, Pen, Highlighter, Marker, Folder, Pen-Stand, Lamp, Mug, Flask, Glass, Jug, Bottle\}. The information about every scene and object is available in  XML and JSON formats. Each scene has a nested list of object data containing \{Position, Orientation, Size, Date and Time of recording, Person ID, Point Indices of the point cloud that have been labelled as belonging to the Object\}. This manual annotation provides the required ground truth data for long term autonomous learning.

\subsection{Dataset Summary}
\label{ssec:Dataset Summary}
This section gives a brief summary of the dataset to serve as a quick reference.

\noindent 3D-TOTAL has:
\begin{itemize}
	\item scenes collected from 20 unique tables, 3 times a day for 19 days and hence in total, approximately 1140 scenes.
	\item each scene manually annotated with 18 possible object classes and an average of 12 object instances, from these classes, annotated per scene.
	\item has annotation stored in XML and JSON formats containing scene instance and it's object instances' specifications.
	\item occurrences of object instances as depicted in Figure~\ref{fig:HistOfObjects} and annotations as exemplified in Figure~\ref{fig:RawPCD},\ref{fig:RawPCDAnnotated}
\end{itemize}

\begin{figure}[t]
\begin{center}
\subfloat[][]{\label{fig:HistOfObjects}\includegraphics[width=0.8\linewidth]{HistOfObjects_crp.pdf}}\\
\subfloat[][]{\label{fig:RawPCD}\includegraphics[width=0.7\linewidth]{pcd.png}}\\
\subfloat[][]{\label{fig:RawPCDAnnotated}\includegraphics[width=0.7\linewidth]{pcd_annotated.png}}
\caption{(a) Objects annotated in 3D Long-Term Dataset, sorted in descending order of count of occurrences. X-axis$=$Object Name, Y-axis$=$Occurrence Count. (b) Screenshot of one table scene, along with it's annotations in (c).}
\end{center}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Analysis}
\label{sec:Analysis}

In this section we perform a first analysis of the data to highlight some ainteresting spects of it. In particular we want to show that 
there are structures in the data that can be exploited by a system for more efficient representations and better reasoning using less data.

Figure ~\ref{fig:Example Scenes} shows three desktop scenes. Each column shows a desk at two different times. The leftmost two columns 
contain scenes from the same day, whereas the third column shows scenes 12 days apart in time. Notice in Column 1: the slight changes in 
position of the keyboard, mouse, papers and pen; Column 2: the relatively big changes in position of laptop, mouse, papers, pen, keyboard, 
lamp. When objects in columns 1,2,3 are compared there is a certain generality in structure (keyboards are always in front of monitors), but 
also a specificity for each person (occurrence of headphones, position of mouse wrt.\ keyboard). Studying the scenes could allow a system to 
infer things related to behaviour of people as well as activity. We can see that there are changes to the first desk suggesting that someone 
was there during and that ths person is quite well-ordered. The second table misses the laptop in the second observation. This suggest that 
the person has left the desk, maybe for the day if it is late. The third desk seems to be occupied by someone that is less sensitive to 
clutter.

As mentioned before, one of our hypotheses is that a qualitative model will be needed to achieve efficient and powerful representations of 
space, at least if the amount of data is limited as it will be in most cases. Such qualitative models could allow some of the inherent 
structure in the environment be encoded in the representation itself. We have already seen in Figure~\ref{fig:Example Scenes} that monitors 
are typically in the back while the keyboard is in front of it. The leftmost table shows an example of a mouse being to the right of a 
keyboard. This is something that we would also expect to hold in many cases. Figure~\ref{fig:scatter-keyboard-mouse} shows a scatter plot 
over the position of keyboard and mouse. The table outline gives an example of a prototypical desk to make it easier to interpret the data. 
In the top part of the figure, the green circles shows the position of the centroid of each 
keyboard that exist in a scene where there is at least one mouse. The red square shows the mean position of all these centroids. The black 
crosses show the position of all mice in scenes with at least one keyboard. In the bottom part of the figure the position of the mouse 
relative to the keyboard is shown. As expected most mice are qualitatively to the right of the keyboard. There are some outliers but simply 
encoding the position of the mouse as being to the right of a keyboard would capture a lot of the information. 

\begin{figure}
\begin{center}
\includegraphics[width=0.8\linewidth]{keyboard_mouse_raw-crop}
\includegraphics[width=0.8\linewidth]{keyboard_mouse-crop}
\end{center}
\caption{Top: The green circles shows the positions keyboards(red square shows mean of keyboards). The black crosses show the position of all mice in these scenes. Bottom: Mean position of keyboards (red square) and relative position of mouse relative to it.} 
\label{fig:scatter-keyboard-mouse}
\end{figure}

To further investigate the correlation between different object classes, and thus look for other inherent structures in the data, we look at 
the relative position of all objects of class $C_j$ w.r.t.\ to another objects of class $C_i$ present in the same scene. To get a quick 
overview of the type of distributions this results in we look at the entropy of these distributions. A large entropy (closer to uniform 
distribution) would suggest that the objects are largely uncorrelated and a small entropy a more peaky distrubution and likely a stronger 
correlation between the objects. We calculate the entropy as 
\begin{equation}
E=-\sum (\frac{n_i}{N})ln(\frac{n_i}{N})
\end{equation}
where $n_i$ is the number of samples that fall in cell $i$ in a grid discretization of the table and N is the total number of samples. Each 
sample correspond to one object pair in one scene. The true entropy will only be estimated well when $N$ is large. We therefore limit this 
investigation to pairs of objects that occur more than a certain number of times in the data. Figure~\ref{fig:entropy} shows these entropies 
for 10 of the objects in the dataset. From this figure we can, for example, see the low entropy in the relation between keyboard and mouse (
element 1,3 and 3,1 in the matrix). The relative positions of monitors and keyboards also have a fairly low entropy. We also see that the 
position of papers is largely uncorrelated with many other objects (uniform distribution gives high entropy). 

In Figure~\ref{fig:scatter-rest} we look closer at some of these relations. Figure~\ref{fig:scatter-monitor-keyboard} shows the position of 
the keyboard w.r.t.\ to the monitors. Our intuition that keyboards are played mostly infront of monitors is supported by data. In Figure~\ref
{fig:scatter-keyboard-mug} shows the position of the mug w.r.t\ the keyboard. We see that the mug is rarely very close to the center of the 
keyboard but rather positioned along a circle around the keyboard. Taking function into acocunt in the analysis could suggest that the mug 
is in fact often placed at arms length from the person working on the desk to keep it far away from the keyboard but still within reach. 
This would mean that the mug as in fact positioned along a circle centered at the shoulder of the person. We see a bias towards the right 
side, a result of most people being right-handed. In Figure~\ref{fig:scatter-keyboard-papers} we see that the position of paper is almost 
completely uncorrelated with the position of the keyboard.

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{entropy_matrix-crop}
\end{center}
\caption{The figure illustrates the entropy in the distribution of relative positions of one objects (column) w.r.t. to another objects (row). Dark red indicates high entropy (more uniform distr.) and dark blue low entropy (peakier distr.). We show this for a subset of objects from the database; 1:keyboard, 2:monitor, 3:mouse, 4:mug, 5:laptop, 6:papers, 7:book, 8:bottle, 9:jug, 10:notebook. }
\label{fig:entropy}
\end{figure}

%\begin{itemize}	
%\item keyboard
%\item monitor
%\item mouse
%\item mug
%\item laptop
%\item papers
%\item book
%\item bottle
%\item jug
%\item notebook
%\item mobile
%\item glass
%\item flask
%\end{itemize}

\begin{figure}
\begin{center}
\subfloat[][]{\label{fig:scatter-monitor-keyboard}\includegraphics[width=0.8\linewidth]{monitor_keyboard-crop}}\quad
\subfloat[][]{\label{fig:scatter-keyboard-mug}\includegraphics[width=0.8\linewidth]{keyboard_mug-crop}}\\
\subfloat[][]{\label{fig:scatter-keyboard-papers}\includegraphics[width=0.8\linewidth]{keyboard_papers-crop}}
\end{center}
\caption{The figures show the relative position of a) keyboard w.r.t. monitor, b) mug w.r.t. keyboard and c) papers w.r.t. keyboard. Qualitatively keyboards are mostly infront of monitors, mugs are around the keyboard and the position of papers is mostly independent on the position of the keyboard.} 
\label{fig:scatter-rest}
\end{figure}

To summarize the analysis, we have shown that the data has many structural properties that a method for representing and reasoning abouts 
pace should make use of. If the aim is to represent typical configuration of objects, this preliminary analysis suggest that a significnat 
part of such knowledge can be encoded well with qualiative spatial relations, such as the mouse is to the right of the keyboard, while 
keeping in mind that this is typical case and not the only possible situation. We can also see that a system that observes these desks for 
an extended period of time will be able to learn quite a lot about the habits of the owners of the desks and even the current activity in 
many cases. 

\subsubsection*{Points Discussed}
\begin{enumerate}
	\item \textbf{figure} Scatter plot for variation within person ID
	\item \textbf{figure} Scatter plot for variation wrt object type
	\item \textbf{figure} Scatter plot for variation considering different landmark - trajectors.
	\item \textbf{figure} Scatter plot of 2D footprints of objects
	\item General conclusions about above figures.
	\item 
	\item 
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
\label{sec:Conclusions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}
\label{sec:Future Work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Points Discussed}
\begin{enumerate}
	\item Room level data set
	\item Which QSR for which purpose?
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgements}
\label{sec:Acknowledgements}
CVAP-KTH, Accel Partners, STRANDS project

\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}